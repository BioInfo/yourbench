task_name: yourbench_y1 # this is the name of your task, and will be used to identify the task in the logs and output files.
configurations:
  push_to_huggingface: true # this determines whether the dataset is pushed to huggingface after generation
  set_hf_repo_visibility: public # this determines the visibility of the huggingface repo.
  # hf_organization: sumukshashidhar-testing # this is the organization to push the dataset to, if push_to_huggingface is true
  hf_organization: sumukshashidhar-testing # this is the organization to push the dataset to, if push_to_huggingface is true
  model:
    # here's an example of how to use azure openai,
    # model_name: gpt-4o-mini
    # model_type: azure
    model_name: deepseek-chat
    max_concurrent_requests: 512
    model_type: openai
    # model_name: meta-llama/Meta-Llama-3.1-8B-Instruct
    # model_name: tiiuae/Falcon3-10B-Instruct
    # model_name: meta-llama/llama-3.1-8b-instruct
    # model_type: openai
    # max_concurrent_requests: 1024
    # max_concurrent_requests: 1

selected_choices: # this is a list of the pipeline parts to be executed.
  generate_dataset:
    execute: false
    files_directory: examples/yourbench_y1 # this is the directory containing the source documents.
    dataset_name: yourbench_y1 # this is the name of the dataset to be created.
  generate_summaries:
    execute: false
    document_dataset_name: yourbench_y1
    summary_dataset_name: yourbench_y1
  create_chunks:
    execute: false
    source_dataset_name: yourbench_y1
    chunked_documents_dataset_name: yourbench_y1_semantically_chunked
    chunking_configuration:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      min_tokens: 256 # tokens
      target_chunk_size: 512 # tokens
      max_tokens: 1024 # tokens
      similarity_threshold: 0.3
      device: cuda # if you have a GPU, you can use it here or set to cpu
  make_multihop_chunks:
    execute: false
    source_dataset_name: yourbench_y1_semantically_chunked
    multihop_pairings_dataset_name: yourbench_y1_multihop_pairings
  create_single_shot_questions:
    execute: false
    source_dataset_name: yourbench_y1_semantically_chunked
    single_shot_questions_dataset_name: yourbench_y1_single_shot_questions_v2
    prompt_prefix: simple_qg
    test_audience: "an expert phd student aiming for a faculty position"
  create_multihop_questions:
    execute: false
    source_dataset_name: yourbench_y1_multihop_pairings
    multihop_questions_dataset_name: yourbench_y1_multihop_questions
    prompt_prefix: simple_qg
    test_audience: "an expert phd student aiming for a faculty position"
  answer_questions_with_llm:
    execute: false
    source_dataset_name: yourbench_y1_single_shot_questions_v2
    prompt_prefix: simple_qg
    answer_scenarios:
      zero_shot:
        execute: true
        answer_dataset_name: yourbench_y1_single_shot_questions_v2x_answers
      zero_shot_with_cot:
        execute: false
        answer_dataset_name: yourbench_y1_single_shot_questions_v2.1_answers
      answer_with_document_summary:
        execute: false
        answer_dataset_name: yourbench_y1_single_shot_questions_v2.1_answers
      answer_with_relevant_chunks:
        execute: false
        answer_dataset_name: yourbench_y1_single_shot_questions_v2.1_answers
      gold_standard:
        execute: true
        answer_dataset_name: yourbench_y1_single_shot_questions_v2x_answers
  reformat_for_judging:
    execute: false
    source_dataset_name: yourbench_y1_single_shot_questions_v2x_answers
    target_dataset_name: yourbench_y1_single_shot_questions_v2x_answers_reformatted
    a:
      model: meta-llama/Meta-Llama-3.1-8B-Instruct
      answer_scenario: zero_shot
    b:
      model: meta-llama/Meta-Llama-3.1-8B-Instruct
      answer_scenario: gold_standard
  judge:
    execute: true
    prompt_prefix: simple_judge
    judge_prompt_name: cot_judge
    source_dataset_name: yourbench_y1_single_shot_questions_v2x_answers_reformatted
    target_dataset_name: yourbench_y1_single_shot_questions_v2x_answers_judged
