# the task name is to be used to identify the task in the logs and output files. you must use a unique name for each task, and pass it as an argument to the yourbench command.
task_name: gpt4_test

# these are special, pipeline wide configurations you can make
configurations:
  # huggingface specific configurations
  huggingface:
    push_to_huggingface: true # this determines whether the dataset is pushed to huggingface after generation
    hf_organization: alozowski # this is the organization to push the dataset to, if push_to_huggingface is true
    set_hf_repo_visibility: private # this determines the visibility of the huggingface repo.
    concat_if_exists: true # this determines whether to concatenate the dataset if it already exists.

  # model specific configurations
  # base urls and api keys will be loaded from the environment variables
  model:
    # here is an example of how to use azure openai's gpt-4o-mini model
    model_name: gpt-4o-mini # your model identifier.
    model_type: openai # this is the type of model you are using. refer to https://docs.litellm.ai/docs/providers for more information.
    max_concurrent_requests: 32 # this is the maximum number of concurrent requests to the model.

# these are where you decide which parts of the pipeline to activate and execute
# to execute each pipeline part, you must set execute to true explicitly
pipeline:
  # this is where we transform source documents into a huggingface dataset
  generate_dataset:
    execute: true
    # this is where the source documents are located
    files_directory: examples/yourbench_y1_preview/
    dataset_name: gpt4_test
  
  # this is where we generate summaries of the documents
  generate_summaries:
    execute: true
    source_dataset_name: gpt4_test
    target_dataset_name: gpt4_test
  
  # this is where we create chunks of the documents, to be used for question generation.
  create_chunks:
    execute: true
    source_dataset_name: gpt4_test
    target_dataset_name: gpt4_test_segmented_single_hop
    chunking_configuration:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      min_tokens: 256 # tokens
      target_chunk_size: 512 # tokens
      max_tokens: 1024 # tokens
      similarity_threshold: 0.3
      device: cpu # if you have a GPU, you can use it here or set to cpu
  
  # this is where we craft these into chunk pairings for multihop question generation
  make_chunk_pairings:
    execute: true
    source_dataset_name: gpt4_test_segmented_single_hop
    target_dataset_name: gpt4_test_segmented_multi_hop
    pairing_configuration:
      min_num_chunks: 2
      max_num_chunks: 5
  
  # this is the question generation step for single hop questions
  create_single_hop_questions:
    execute: true
    source_dataset_name: gpt4_test_segmented_single_hop
    target_dataset_name: gpt4_test_single_hop_questions
    prompt_prefix: simple_qg
    test_audience: "an expert in the field"
  
  # this is the question generation step for multihop questions
  create_multi_hop_questions:
    execute: true
    source_dataset_name: gpt4_test_segmented_multi_hop
    target_dataset_name: gpt4_test_multi_hop_questions
    prompt_prefix: simple_qg
    test_audience: "an expert in the field"
  
  # perform re-weighting and deduplication of questions here
  reweight_and_deduplicate_questions:
    execute: true
    source_single_hop_questions_dataset_name: gpt4_test_single_hop_questions
    source_multi_hop_questions_dataset_name: gpt4_test_multi_hop_questions
    large_question_dataset_name: gpt4_test_questions_large
    target_dataset_name: gpt4_test_questions
    cluster_configuration:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      alpha: 0.7
      eps: 0.20
      batch_size: 128
      distance_threshold: 0.7 # this is the distance threshold for clustering. it needs to be tuned well.
      top_k: 10 # number of neighbors to search for each vector
      min_samples: 1
      output_file: ./logs/deduplication_results.txt
      # these are for picking representative questions and answers
      lambda_val: 1.0
      w_max: 5.0
  
  # this is where we answer questions with llm
  answer_questions_with_llm:
    execute: true
    source_dataset_name: gpt4_test_questions
    target_dataset_name: gpt4_test_questions_with_answers
    prompt_prefix: simple_qg
    answer_scenarios:
      zero_shot:
        execute: true
        answer_dataset_name: gpt4_test_questions_with_answers
      zero_shot_with_cot:
        execute: true
        answer_dataset_name: gpt4_test_questions_with_answers
      answer_with_document_summary:
        execute: true
        answer_dataset_name: gpt4_test_questions_with_answers
      answer_with_relevant_chunks:
        execute: true
        answer_dataset_name: gpt4_test_questions_with_answers
      gold_standard:
        execute: true
        answer_dataset_name: gpt4_test_questions_with_answers
  
  # this is where we reformat the dataset for the judge
  reformat_for_judging:
    execute: true
    source_dataset_name: gpt4_test_questions_with_answers
    target_dataset_name: gpt4_test_questions_with_answers_for_judge
    a:
      model: gpt-4o-mini
      answer_scenario: zero_shot
    b:
      model: gpt-4o-mini
      answer_scenario: gold_standard
  
  # this is where we judge the answers
  judge:
    execute: true
    source_dataset_name: gpt4_test_questions_with_answers_for_judge
    target_dataset_name: gpt4_test_questions_with_answers_for_judge_judged
    prompt_prefix: simple_judge
    judge_prompt_name: cot_judge
  
  # this is where we visualize the judge results
  visualize_results:
    execute: true
    source_dataset_name: gpt4_test_questions_with_answers_for_judge_judged
    plots_directory: ./logs/plots/

