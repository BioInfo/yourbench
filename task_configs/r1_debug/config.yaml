task_name: r1_debug

# Special configurations
configurations:
  huggingface:
    push_to_huggingface: true
    hf_organization: alozowski
    set_hf_repo_visibility: private
    concat_if_exists: false

  model:
    model_name: openrouter/deepseek/deepseek-r1
    api_base_url: https://openrouter.ai/api/v1
    max_concurrent_requests: 16

# Only executing answer generation step
pipeline:
  generate_dataset:
    execute: true
    files_directory: examples/yourbench_y1
    dataset_name: r1_debug

  generate_summaries:
    execute: true
    source_dataset_name: r1_debug
    target_dataset_name: r1_debug

  create_chunks:
    execute: false
    source_dataset_name: r1_debug
    target_dataset_name: r1_debug_segmented_single_hop
    chunking_configuration:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      min_tokens: 256 # tokens
      target_chunk_size: 512 # tokens
      max_tokens: 1024 # tokens
      similarity_threshold: 0.3
      device: cpu # if you have a GPU, you can use it here or set to cpu
    
  make_chunk_pairings:
    execute: false
    source_dataset_name: r1_debug_segmented_single_hop
    target_dataset_name: r1_debug_segmented_multi_hop
    pairing_configuration:
      min_num_chunks: 2
      max_num_chunks: 5
  
  create_single_hop_questions:
    execute: false
    source_dataset_name: r1_debug_segmented_single_hop
    target_dataset_name: r1_debug_single_hop_questions
    prompt_prefix: simple_qg
    test_audience: "an expert in the field"
    
  create_multi_hop_questions:
    execute: false
    source_dataset_name: r1_debug_segmented_multi_hop
    target_dataset_name: r1_debug_multi_hop_questions
    prompt_prefix: simple_qg
    test_audience: "an expert in the field"

  reweight_and_deduplicate_questions:
    execute: false
    source_single_hop_questions_dataset_name: r1_debug_single_hop_questions
    source_multi_hop_questions_dataset_name: r1_debug_multi_hop_questions
    large_question_dataset_name: r1_debug_questions_large
    target_dataset_name: r1_debug_questions
    cluster_configuration:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      alpha: 0.7
      eps: 0.20
      batch_size: 128
      distance_threshold: 0.7 # this is the distance threshold for clustering. it needs to be tuned well.
      top_k: 10 # number of neighbors to search for each vector
      min_samples: 1
      output_file: ./logs/deduplication_results.txt
      # these are for picking representative questions and answers
      lambda_val: 1.0
      w_max: 5.0

  # this is where we answer questions with llm
  answer_questions_with_llm:
    execute: false
    source_dataset_name: r1_debug_questions
    target_dataset_name: r1_debug_questions_with_answers
    prompt_prefix: simple_qg
    answer_scenarios:
      zero_shot:
        execute: false
        answer_dataset_name: r1_debug_questions_with_answers
      zero_shot_with_cot:
        execute: false
        answer_dataset_name: r1_debug_questions_with_answers
      answer_with_document_summary:
        execute: false
        answer_dataset_name: r1_debug_questions_with_answers
      answer_with_relevant_chunks:
        execute: false
        answer_dataset_name: r1_debug_questions_with_answers
      gold_standard:
        execute: false
        answer_dataset_name: r1_debug_questions_with_answers

