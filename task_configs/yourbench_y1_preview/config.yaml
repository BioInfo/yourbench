# the task name is to be used to identify the task in the logs and output files. you must use a unique name for each task, and pass it as an argument to the yourbench command.
task_name: yourbench_y1_preview

# these are special, pipeline wide configurations you can make
configurations:
  # huggingface specific configurations
  huggingface:
    push_to_huggingface: true # this determines whether the dataset is pushed to huggingface after generation
    hf_organization: sumuks # this is the organization to push the dataset to, if push_to_huggingface is true
    set_hf_repo_visibility: private # this determines the visibility of the huggingface repo.
    concat_if_exists: false # this determines whether to concatenate the dataset if it already exists.

  # model specific configurations
  # base urls and api keys will be loaded from the environment variables
  model:
    # here is an example of how to use azure openai's gpt-4o-mini model
    model_name: gpt-4o # your model identifier.
    model_type: azure # this is the type of model you are using. refer to https://docs.litellm.ai/docs/providers for more information.
    max_concurrent_requests: 8 # this is the maximum number of concurrent requests to the model.
  
# these are where you decide which parts of the pipeline to activate and execute
# to execute each pipeline part, you must set execute to true explicitly
pipeline:
  # this is where we transform source documents into a huggingface dataset
  generate_dataset:
    execute: false
    # this is where the source documents are located
    files_directory: examples/yourbench_y1_preview/
    dataset_name: yourbench_y1_preview
  
  # this is where we generate summaries of the documents
  generate_summaries:
    execute: false
    source_dataset_name: yourbench_y1_preview
    target_dataset_name: yourbench_y1_preview
  
  # this is where we create chunks of the documents, to be used for question generation.
  create_chunks:
    execute: false
    source_dataset_name: yourbench_y1_preview
    target_dataset_name: yourbench_y1_preview_segmented_single_hop
    chunking_configuration:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      min_tokens: 256 # tokens
      target_chunk_size: 512 # tokens
      max_tokens: 1024 # tokens
      similarity_threshold: 0.3
      device: cuda # if you have a GPU, you can use it here or set to cpu
  
  # this is where we craft these into chunk pairings for multihop question generation
  make_chunk_pairings:
    execute: false
    source_dataset_name: yourbench_y1_preview_segmented_single_hop
    target_dataset_name: yourbench_y1_preview_segmented_multi_hop
    pairing_configuration:
      min_num_chunks: 2
      max_num_chunks: 5
  
  # this is the question generation step for single hop questions
  create_single_hop_questions:
    execute: false
    source_dataset_name: yourbench_y1_preview_segmented_single_hop
    target_dataset_name: yourbench_y1_preview_single_hop_questions
    prompt_prefix: simple_qg
    test_audience: "an expert in the field"
  
  # this is the question generation step for multihop questions
  create_multi_hop_questions:
    execute: false
    source_dataset_name: yourbench_y1_preview_segmented_multi_hop
    target_dataset_name: yourbench_y1_preview_multi_hop_questions
    prompt_prefix: simple_qg
    test_audience: "an expert in the field"
  
  # perform re-weighting and deduplication of questions here
  reweight_and_deduplicate_questions:
    execute: true
    source_single_hop_questions_dataset_name: yourbench_y1_preview_single_hop_questions
    source_multi_hop_questions_dataset_name: yourbench_y1_preview_multi_hop_questions
    large_question_dataset_name: yourbench_y1_preview_complete_questions
    target_dataset_name: yourbench_y1_preview_questions
    cluster_configuration:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      alpha: 0.7
      batch_size: 128
      distance_threshold: 0.7 # this is the distance threshold for clustering. it needs to be tuned well.
      top_k: 10 # number of neighbors to search for each vector

      # these are for picking representative questions and answers
      lambda_val: 1.0
      w_max: 5.0